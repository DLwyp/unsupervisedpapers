---
{
  "area": "natural-language-processing",
  "title": "Latent Dirichlet allocation",
  "year": null,
  "categories": [
    "generative-models"
  ],
  "components": [],
  "introduced_by": null,
  "also_known_as": [
    "LDA"
  ],
  "links": [
    {
      "title": "Latent Dirichlet Allocation",
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/Latent-Dirichlet-Allocation-Blei-Ng/f198043a866e9187925a8d8db9a55e3bfdd47f2c"
    },
    {
      "resource": "Wikipedia",
      "icon": "wikipedia",
      "url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"
    },
    {
      "resource": "Towards Data Science",
      "icon": "website",
      "url": "https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2"
    },
    {
      "resource": "Towards Data Science",
      "icon": "website",
      "url": "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"
    }
  ],
  "thumbnail": "latent-dirichlet-allocation-thumb.jpg",
  "card": "latent-dirichlet-allocation-card.jpg"
}
---
Latent Dirichlet allocation(LDA), is a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which eachitem of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document.  

Source, Image Source: [Latent Dirichlet Allocation](https://www.semanticscholar.org/paper/Latent-Dirichlet-Allocation-Blei-Ng/f198043a866e9187925a8d8db9a55e3bfdd47f2c) [[PDF](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)]  
