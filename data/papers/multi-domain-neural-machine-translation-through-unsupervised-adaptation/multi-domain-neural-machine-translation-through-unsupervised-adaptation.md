---
{
  "title": "Multi-Domain Neural Machine Translation through Unsupervised Adaptation",
  "authors": [
    "M. A. Farajian",
    "M. Turchi",
    "M. Negri",
    "M. Federico"
  ],
  "abstract": "We investigate the application of Neural Machine Translation (NMT) under the following three conditions posed by realworld application scenarios. First, we operate with an input stream of sentences coming from many different domains and with no predefined order. Second, the sentences are presented without domain information. Third, the input stream should be processed by a single generic NMT model. To tackle the weaknesses of current NMT technology in this unsupervised multi-domain setting, we explore an efficient instance-based adaptation method that, by exploiting the similarity between the training instances and each test sentence, dynamically sets the hyperparameters of the learning algorithm and updates the generic model on-the-fly. The results of our experiments with multi-domain data show that local adaptation outperforms not only the original generic NMT system, but also a strong phrase-based system and even single-domain NMT models specifically optimized on each domain and applicable only by violating two of our aforementioned assumptions.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://pdfs.semanticscholar.org/0b13/17a42760d4de86d721a867e340db4d6b0810.pdf"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/0b1317a42760d4de86d721a867e340db4d6b0810"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "multi-domain-neural-machine-translation-through-unsupervised-adaptation-thumb.jpg",
  "card": "multi-domain-neural-machine-translation-through-unsupervised-adaptation-card.jpg",
  "s2_paper_id": "0b1317a42760d4de86d721a867e340db4d6b0810"
}
---

