---
{
  "title": "Variational Gaussian Process",
  "date": "2015-11-20",
  "authors": [
    "Dustin Tran",
    "R. Ranganath",
    "D. Blei"
  ],
  "abstract": "Abstract: Representations offered by deep generative models are fundamentally tied to their inference method from data. Variational inference methods require a rich family of approximating distributions. We construct the variational Gaussian process (VGP), a Bayesian nonparametric model which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by autoencoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://arxiv.org/pdf/1511.06499.pdf"
    },
    {
      "resource": "arXiv.org",
      "icon": "arxiv",
      "url": "https://arxiv.org/abs/1511.06499"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/ed032736652ac7e1f36ea17bd253cd1bfdcc3864"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "variational-gaussian-process-thumb.jpg",
  "card": "variational-gaussian-process-card.jpg",
  "s2_paper_id": "ed032736652ac7e1f36ea17bd253cd1bfdcc3864"
}
---

