---
{
  "title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",
  "date": "2018-03-23",
  "authors": [
    "Yuxuan Wang",
    "Daisy Stanton",
    "Yu Zhang",
    "R. Skerry-Ryan",
    "Eric Battenberg",
    "Joel Shor",
    "Y. Xiao",
    "F. Ren",
    "Ye Jia",
    "R. A. Saurous"
  ],
  "abstract": "In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable \"labels\" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",
  "links": [
    {
      "title": "PDF",
      "type": "pdf",
      "url": "https://arxiv.org/pdf/1803.09017.pdf"
    },
    {
      "title": "arXiv.org",
      "type": "arxiv",
      "url": "https://arxiv.org/abs/1803.09017"
    },
    {
      "title": "Semantic Scholar",
      "type": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/1db4d7cf784e53f62ffe414bbde2cdcd770e579f"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "style-tokens-unsupervised-style-modeling-control-and-transfer-in-end-to-end-speech-synthesis-thumb.jpg",
  "card": "style-tokens-unsupervised-style-modeling-control-and-transfer-in-end-to-end-speech-synthesis-card.jpg",
  "s2_paper_id": "1db4d7cf784e53f62ffe414bbde2cdcd770e579f"
}
---

