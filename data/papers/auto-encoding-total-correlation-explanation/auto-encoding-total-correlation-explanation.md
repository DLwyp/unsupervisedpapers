---
{
  "title": "Auto-Encoding Total Correlation Explanation",
  "date": "2018-02-16",
  "authors": [
    "Shuyang Gao",
    "Rob Brekelmans",
    "G. V. Steeg",
    "A. Galstyan"
  ],
  "abstract": "Advances in unsupervised learning enable reconstruction and generation of samples from complex distributions, but this success is marred by the inscrutability of the representations learned. We propose an information-theoretic approach to characterizing disentanglement and dependence in representation learning using multivariate mutual information, also called total correlation. The principle of total Cor-relation Ex-planation (CorEx) has motivated successful unsupervised learning applications across a variety of domains, but under some restrictive assumptions. Here we relax those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly, we find that this lower bound is equivalent to the one in variational autoencoders (VAE) under certain conditions. This information-theoretic view of VAE deepens our understanding of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent codes more interpretable through information maximization and enables generation of richer and more realistic samples.",
  "links": [
    {
      "title": "PDF",
      "type": "pdf",
      "url": "https://arxiv.org/pdf/1802.05822.pdf"
    },
    {
      "title": "arXiv.org",
      "type": "arxiv",
      "url": "https://arxiv.org/abs/1802.05822"
    },
    {
      "title": "Semantic Scholar",
      "type": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/18601d76ec363efd8a7b4eca34c8f0f6b001500b"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "auto-encoding-total-correlation-explanation-thumb.jpg",
  "card": "auto-encoding-total-correlation-explanation-card.jpg",
  "s2_paper_id": "18601d76ec363efd8a7b4eca34c8f0f6b001500b"
}
---

