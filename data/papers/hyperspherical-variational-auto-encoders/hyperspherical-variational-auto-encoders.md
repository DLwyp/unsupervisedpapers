---
{
  "title": "Hyperspherical Variational Auto-Encoders",
  "date": "2018-04-03",
  "authors": [
    "T. Davidson",
    "Luca Falorsi",
    "Nicola De Cao",
    "Thomas Kipf",
    "J. Tomczak"
  ],
  "abstract": "The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or $\\mathcal{S}$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, $\\mathcal{N}$-VAE, in low dimensions on other data types.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://arxiv.org/pdf/1804.00891.pdf"
    },
    {
      "resource": "arXiv.org",
      "icon": "arxiv",
      "url": "https://arxiv.org/abs/1804.00891"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/5f90bfbb93cd3c36e550a8aacc400a6c3f0a7c7e"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "hyperspherical-variational-auto-encoders-thumb.jpg",
  "card": "hyperspherical-variational-auto-encoders-card.jpg",
  "s2_paper_id": "5f90bfbb93cd3c36e550a8aacc400a6c3f0a7c7e"
}
---

