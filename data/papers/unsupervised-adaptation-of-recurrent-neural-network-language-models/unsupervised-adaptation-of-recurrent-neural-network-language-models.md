---
{
  "title": "Unsupervised Adaptation of Recurrent Neural Network Language Models",
  "date": "2016-09-08",
  "authors": [
    "Siva Reddy Gangireddy",
    "P. Swietojanski",
    "P. Bell",
    "S. Renals"
  ],
  "abstract": "Recurrent neural network language models (RNNLMs) have been shown to consistently improve Word Error Rates (WERs) of large vocabulary speech recognition systems employing ngram LMs. In this paper we investigate supervised and unsupervised discriminative adaptation of RNNLMs in a broadcast transcription task to target domains defined by either genre or show. We have explored two approaches based on (1) scaling forward-propagated hidden activations (Learning Hidden Unit Contributions (LHUC) technique) and (2) direct fine-tuning of the parameters of the whole RNNLM. To investigate the effectiveness of the proposed methods we carry out experiments on multi-genre broadcast (MGB) data following the MGB-2015 challenge protocol. We observe small but significant improvements in WER compared to a strong unadapted RNNLM model.",
  "links": [
    {
      "title": "PDF",
      "type": "pdf",
      "url": "https://pdfs.semanticscholar.org/59b9/4b938bbc40b1864ccf73924f37e63f684b00.pdf"
    },
    {
      "title": "Semantic Scholar",
      "type": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/9ae3f23cbe1f32abc644d0edf0522b18ee6ec39f"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "unsupervised-adaptation-of-recurrent-neural-network-language-models-thumb.jpg",
  "card": "unsupervised-adaptation-of-recurrent-neural-network-language-models-card.jpg",
  "s2_paper_id": "9ae3f23cbe1f32abc644d0edf0522b18ee6ec39f"
}
---

