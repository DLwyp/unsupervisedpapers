---
{
  "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
  "date": "2017-03-15",
  "authors": [
    "Sainbayar Sukhbaatar",
    "Ilya Kostrikov",
    "Arthur Szlam",
    "R. Fergus"
  ],
  "abstract": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://arxiv.org/pdf/1703.05407.pdf"
    },
    {
      "resource": "arXiv.org",
      "icon": "arxiv",
      "url": "https://arxiv.org/abs/1703.05407"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/8499a250422a3c66357367c8d5fa504de5424c59"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "intrinsic-motivation-and-automatic-curricula-via-asymmetric-self-play-thumb.jpg",
  "card": "intrinsic-motivation-and-automatic-curricula-via-asymmetric-self-play-card.jpg",
  "s2_paper_id": "8499a250422a3c66357367c8d5fa504de5424c59"
}
---

