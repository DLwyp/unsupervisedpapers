---
{
  "title": "Unsupervised Pretraining for Sequence to Sequence Learning",
  "date": "2016-11-08",
  "authors": [
    "Prajit Ramachandran",
    "Peter J. Liu",
    "Quoc V. Le"
  ],
  "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models. Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.",
  "links": [
    {
      "title": "PDF",
      "type": "pdf",
      "url": "https://arxiv.org/pdf/1611.02683.pdf"
    },
    {
      "title": "arXiv.org",
      "type": "arxiv",
      "url": "https://arxiv.org/abs/1611.02683"
    },
    {
      "title": "Semantic Scholar",
      "type": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/85f94d8098322f8130512b4c6c4627548ce4a6cc"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "unsupervised-pretraining-for-sequence-to-sequence-learning-thumb.jpg",
  "card": "unsupervised-pretraining-for-sequence-to-sequence-learning-card.jpg",
  "s2_paper_id": "85f94d8098322f8130512b4c6c4627548ce4a6cc"
}
---

