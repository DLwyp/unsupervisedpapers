---
{
  "title": "SciBERT: A Pretrained Language Model for Scientific Text",
  "date": "2019-03-26",
  "authors": [
    "Iz Beltagy",
    "Kyle Lo",
    "Arman Cohan"
  ],
  "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https URL.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://pdfs.semanticscholar.org/1794/8fa14d349d6d62d7c8db9192387fdbf46d20.pdf"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/5e98fe2163640da8ab9695b9ee9c433bb30f5353"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "sci-bert-a-pretrained-language-model-for-scientific-text-thumb.jpg",
  "card": "sci-bert-a-pretrained-language-model-for-scientific-text-card.jpg",
  "s2_paper_id": "5e98fe2163640da8ab9695b9ee9c433bb30f5353"
}
---

