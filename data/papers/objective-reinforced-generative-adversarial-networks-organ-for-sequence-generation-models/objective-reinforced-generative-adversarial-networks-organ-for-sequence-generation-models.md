---
{
  "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
  "date": "2017-05-30",
  "authors": [
    "G. L. Guimaraes",
    "Benjamin Sanchez-Lengeling",
    "Pedro Luis Cunha Farias",
    "A. Aspuru-Guzik"
  ],
  "abstract": "In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://arxiv.org/pdf/1705.10843.pdf"
    },
    {
      "resource": "arXiv.org",
      "icon": "arxiv",
      "url": "https://arxiv.org/abs/1705.10843"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/15d739e2c184a6844bdbd9a2550d007de6ddb085"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "objective-reinforced-generative-adversarial-networks-organ-for-sequence-generation-models-thumb.jpg",
  "card": "objective-reinforced-generative-adversarial-networks-organ-for-sequence-generation-models-card.jpg",
  "s2_paper_id": "15d739e2c184a6844bdbd9a2550d007de6ddb085"
}
---

