---
{
  "title": "Learning deep representations by mutual information estimation and maximization",
  "date": "2018-08-20",
  "authors": [
    "R. D. Hjelm",
    "A. Fedorov",
    "Samuel Lavoie-Marchildon",
    "Karan Grewal",
    "Adam Trischler",
    "Y. Bengio"
  ],
  "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://arxiv.org/pdf/1808.06670.pdf"
    },
    {
      "resource": "arXiv.org",
      "icon": "arxiv",
      "url": "https://arxiv.org/abs/1808.06670"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/eae7d5b15423a148e6bb32d24bbabedfacd0e2df"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "learning-deep-representations-by-mutual-information-estimation-and-maximization-thumb.jpg",
  "card": "learning-deep-representations-by-mutual-information-estimation-and-maximization-card.jpg",
  "s2_paper_id": "eae7d5b15423a148e6bb32d24bbabedfacd0e2df"
}
---

