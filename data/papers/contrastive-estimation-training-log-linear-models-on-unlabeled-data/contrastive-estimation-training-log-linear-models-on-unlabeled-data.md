---
{
  "title": "Contrastive Estimation: Training Log-Linear Models on Unlabeled Data",
  "date": "2005-06-25",
  "authors": [
    "Noah A. Smith",
    "J. Eisner"
  ],
  "abstract": "Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem---POS tagging given a tagging dictionary and unlabeled text---contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "http://www.cs.cmu.edu/~nasmith/papers/smith+eisner.acl05.pdf"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/9452e711ce2e7e0d4e35aaeb5ab8731de62a5809"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "contrastive-estimation-training-log-linear-models-on-unlabeled-data-thumb.jpg",
  "card": "contrastive-estimation-training-log-linear-models-on-unlabeled-data-card.jpg",
  "s2_paper_id": "9452e711ce2e7e0d4e35aaeb5ab8731de62a5809"
}
---

