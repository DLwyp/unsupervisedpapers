---
{
  "title": "Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation",
  "date": "2019-07-28",
  "authors": [
    "Haipeng Sun",
    "Rui Wang",
    "Kehai Chen",
    "M. Utiyama",
    "Eiichiro Sumita",
    "T. Zhao"
  ],
  "abstract": "Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using nonparallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT.",
  "links": [
    {
      "resource": "PDF",
      "icon": "pdf",
      "url": "https://pdfs.semanticscholar.org/58af/f08ffa4db37f0a965b9f0e33c7c24cf34c16.pdf"
    },
    {
      "resource": "Semantic Scholar",
      "icon": "semanticscholar",
      "url": "https://www.semanticscholar.org/paper/58aff08ffa4db37f0a965b9f0e33c7c24cf34c16"
    }
  ],
  "supervision": [],
  "tasks": [],
  "methods": [],
  "thumbnail": "unsupervised-bilingual-word-embedding-agreement-for-unsupervised-neural-machine-translation-thumb.jpg",
  "card": "unsupervised-bilingual-word-embedding-agreement-for-unsupervised-neural-machine-translation-card.jpg",
  "s2_paper_id": "58aff08ffa4db37f0a965b9f0e33c7c24cf34c16"
}
---

